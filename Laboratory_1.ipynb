{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOOFeE0YaQmUvchsiQg+8E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndromedaOMA/Advanced_Analytics_with_Apache_Spark---E.On_Software_Development/blob/main/Laboratory_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark oferă:\n",
        "    \n",
        "1.  Un obiect de tip **Spark Context**, folosit pentru interacțiunea cu Spark.\n",
        "2.  Un obiect de bază pentru codul SQL și citirea datelor, numit **Spark Session**.\n",
        "3.  Un obiect de lucru cu date, numit **Data Frame**, având o interfață similară cu cel de *Pandas*.\n",
        "4.  Funcții gata implementate pentru transformări și expresii.\n",
        "5.   Abilitatea de a construi propriile funcții de transformare în *Python*.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q5rBoT1tEDhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **Pregătire mediu de lucru**\n",
        "\n",
        "Stabilirea conexiunii dintre Google drive si Colab notebook și importarea/instalarea pachetelor și librăriilor necesare mai departe."
      ],
      "metadata": {
        "id": "NYNy-9DFFGuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m84rf6UjB9Z8",
        "outputId": "bdafaa41-d3ee-4b2d-cca0-65e96d8e970c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://dlcdn.apache.org/spark/\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.4.4/spark-3.4.4-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.4-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.4-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "import pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qs8o6ObCu8q",
        "outputId": "aa15e74b-2d6c-4f81-db55-fa17c287ffe8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "111 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "spark= SparkSession \\\n",
        ".builder \\\n",
        ".getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "TOZYMlKvCv4M",
        "outputId": "90995888-15f8-41d5-a7ba-65d2f7c784ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79ea7031c610>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2e6ccaef6c66:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **Data Frame**\n",
        "\n",
        "**Spark SQL** lucrează datele sub forma unui **tabel**.\n",
        "\n",
        "Librăria **PySpark** oferă un clasă care reprezintă un tabel de date, unde fiecare coloană conține același tip de date, numită **Data Frame**, similar cu ce oferă librăria **Pandas**.\n",
        "\n",
        "Fiind concepute pentru Big Data, un Data Frame nu încarcă datele.\n",
        "El reține doar locația și tipul lor.\n",
        "La fiecare  transformare de date, noul Data Frame va fi creat care reține locația și tipul datelor, precum și lanțul de transformări.\n",
        "Transformările vor fi efectuate **doar** în momentul scrierii sau afișării datelor de către executori."
      ],
      "metadata": {
        "id": "o8HCp868KDX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ➢ Această schema se poate defini de dezolator după un anume format\n",
        "\n",
        " ➢ Pentru a dezactiva determinarea automata a tipurilor de date, la citire, se poate furniza schema\n",
        "\n",
        " ➢ Principalele tipuri de date din Spark (Lista completă: https://spark.apache.org/docs/latest/sql-ref-datatypes.html ):\n",
        "\n",
        "1. integer\n",
        "2. string\n",
        "3. float\n",
        "4. double\n",
        "5. boolean\n",
        "6. date\n",
        "7. timestamp\n",
        "8. array<tip element>\n",
        "9. map<tip cheie, tip valoare>\n",
        "10. struct<coloana1:tip 1, coloana2:tip 2,...>"
      ],
      "metadata": {
        "id": "FE7GEOciWEKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crearea unui Data Frame Spark dintr-o listă de Python."
      ],
      "metadata": {
        "id": "IUWlnNw-Fu4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    ['Vali', 23, 'Programator', 4, None, ['Sport', 'Boardgames']],\n",
        "    ['Vlad', 34, 'Instalator', 11, None, ['Alergare']],\n",
        "    ['Bea', 29, 'Reporter', 7, True, None]\n",
        "]\n",
        "\n",
        "data_df_python = spark.createDataFrame(data)\n",
        "\n",
        "data_df_python.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ27FyYXDEAq",
        "outputId": "7a45ada5-47e1-45b4-dde9-4cdc77ec83fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----------+---+----+-------------------+\n",
            "|  _1| _2|         _3| _4|  _5|                 _6|\n",
            "+----+---+-----------+---+----+-------------------+\n",
            "|Vali| 23|Programator|  4|NULL|[Sport, Boardgames]|\n",
            "|Vlad| 34| Instalator| 11|NULL|         [Alergare]|\n",
            "| Bea| 29|   Reporter|  7|true|               NULL|\n",
            "+----+---+-----------+---+----+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formatele JSON, Parquet și CSV\n",
        "\n",
        "În Big Data, în general, datele sunt stocate extern în unul sau mai multe formate. PySpark oferă metode\n",
        "pentru citirea celor mai populare și des utilizare formate, precum JSON, Parquet sau CSV."
      ],
      "metadata": {
        "id": "VTYxHFt_LoLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Citirea fișierelor de tip JSON Lines."
      ],
      "metadata": {
        "id": "8tpt9_gxGbPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_json = \"/content/drive/MyDrive/FP-EON/practice/IN/json\"\n",
        "data_df_json = spark.read.format('json').load(path_json)\n",
        "data_df_json.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTRg-VhZF-Mk",
        "outputId": "af7bffc9-ae26-4920-e2d8-883503a26d17"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+-----------+--------------------+------+-------+\n",
            "|               extra|inactiv|       nume|            ocupatie|varsta|vechime|\n",
            "+--------------------+-------+-----------+--------------------+------+-------+\n",
            "|            [PV, EV]|   NULL|     Andrei|Specialist marketing|    38|     13|\n",
            "|   [3D Printer, WII]|   NULL|  Alexandru|      Specialist HR |    34|      8|\n",
            "| [AC, EV, 5G Router]|   NULL|     Adrian|       Inginer civil|    45|     23|\n",
            "|              [XBOX]|   NULL|      Alin |   Vânzător  retail |    26|      2|\n",
            "|[5G Router, 3D Pr...|   NULL|      Anton|     Manager proiect|    40|     15|\n",
            "|            [EV, AC]|   NULL|        Ana|  Muncitor alimentar|    35|      9|\n",
            "|[PC, 3D Printer, AC]|  false|     Bogdan|           Farmacist|    50|     32|\n",
            "|                NULL|   true|  Cătălin  |        Medic primar|    20|      0|\n",
            "|      [AC, XBOX, PV]|   NULL|     Cosmin|           Farmacist|    49|     31|\n",
            "|[XBOX, 3D Printer...|  false|   Cristian|    Asistent  social|    44|     21|\n",
            "|[3D Printer, 3D P...|   NULL|    Gabriel|     Asistent social|    44|     21|\n",
            "|[3D Printer, AC, ...|  false|     George|     Agent imobiliar|    46|     25|\n",
            "|   [WII, 3D Printer]|   NULL|   Gheorghe|      Specialist HR |    34|      8|\n",
            "|[WII, 5G Router, ...|   NULL|    Grigore|    Asistent  social|    44|     21|\n",
            "|     [5G Router, PV]|   NULL|      Horia|           Arhitect |    42|     18|\n",
            "|                [PC]|   NULL|      Ilie |     Vânzător retail|    26|      2|\n",
            "|              [XBOX]|   NULL|       Ion |     Vânzător retail|    27|      2|\n",
            "|[EV, XBOX, 3D Pri...|   NULL|      Ionel|     Inginer  civil |    45|     23|\n",
            "|                NULL|   NULL|     Iosif |   Vânzător  retail |    26|      2|\n",
            "|      [PV, XBOX, AC]|  false|     Lucian|     Designer grafic|    49|     30|\n",
            "+--------------------+-------+-----------+--------------------+------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Citirea fișierelor de tip Parquet."
      ],
      "metadata": {
        "id": "v3diZlQ6IVFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_parquet = \"/content/drive/MyDrive/FP-EON/practice/IN/parquet\"\n",
        "data_df_parquet = spark.read.format('parquet').load(path_parquet)\n",
        "data_df_parquet.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYm0ZnAGISm9",
        "outputId": "c62b7700-52c6-440c-c66b-194206c72697"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------+--------------------+-------+-------+--------------------+\n",
            "|        nume|varsta|            ocupatie|vechime|inactiv|               extra|\n",
            "+------------+------+--------------------+-------+-------+--------------------+\n",
            "|       Daria|    33|   Inginer  mecanic |      7|   NULL|                [EV]|\n",
            "|     Delia  |    20|        Medic primar|      0|   true|                NULL|\n",
            "|       Diana|    45|   Asistent  social |     23|   NULL|[XBOX, 3D Printer...|\n",
            "|       Doina|    37|Specialist  marke...|     11|   NULL|          [WII, PS5]|\n",
            "|      Elena |    25|            Contabil|      1|   NULL|                NULL|\n",
            "|     Eliza  |    23|          Instalator|      0|   NULL|                NULL|\n",
            "|     Emilia |    25|            Contabil|      1|   NULL|                NULL|\n",
            "|   Eugenia  |    24|        Programator |      1|   NULL|                NULL|\n",
            "|     Felicia|    34|      Specialist  HR|      8|   NULL|          [WII, PS5]|\n",
            "|    Florina |    28|               Șofer|      3|   NULL|                [EV]|\n",
            "|      Vasile|    40|     Manager proiect|     15|   NULL|            [AC, AC]|\n",
            "|      Victor|    42|            Arhitect|     18|   NULL|   [5G Router, XBOX]|\n",
            "|      Viorel|    50|          Farmacist |     32|  false|[PS5, 3D Printer,...|\n",
            "|  Vladimir  |    18|   Asistent  medical|      0|   true|                NULL|\n",
            "|     Voicu  |    22|          Instalator|      0|   NULL|                NULL|\n",
            "|      Zamfir|    46|       Inginer civil|     24|   NULL|[3D Printer, 3D P...|\n",
            "|  Alexandra |    29|Operator call center|      4|  false|                [PC]|\n",
            "|         Ana|    45|     Asistent social|     22|   NULL|[3D Printer, 5G R...|\n",
            "|       Alina|    44|    Asistent  social|     21|   NULL|      [PS5, AC, PS5]|\n",
            "|       Adela|    40|    Manager  proiect|     15|   NULL|    [3D Printer, PC]|\n",
            "+------------+------+--------------------+-------+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Citirea fișierelor de tip CSV cu rând de Header."
      ],
      "metadata": {
        "id": "-baDcrDZJFnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putem schimba și setările implicite de citire. De exemplu, implicit, Spark ignoră Header-ul din fișiere CSV."
      ],
      "metadata": {
        "id": "yCORGYmcJEiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_csv = 'path/to/folder/or/file'\n",
        "data_df_csv = spark.read.format('csv').option('header','true').load(path_csv)"
      ],
      "metadata": {
        "id": "wXqgE_gAIslB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Schema datelor**\n",
        "\n",
        "Întrucât Spark SQL a fost conceput pentru date structurate, el încearcă să determine tipurile de coloane, numită schema datelor, în mod automat, din sursele de date la citire. Acest proces nu este necesar pentru formate structurate ca Parquet, dar necesar pentru obiecte Python sau date semi-structurate, JSON și CSV."
      ],
      "metadata": {
        "id": "5DM5LKwyKd-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definirea unei scheme a datelor"
      ],
      "metadata": {
        "id": "JqAQej7FQwnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark oferă un și un mod programatic de a defini o schemă a datelor:"
      ],
      "metadata": {
        "id": "QyO6IqktSDI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import types as types\n",
        "\n",
        "data_schema = T.StructType([\n",
        "    T.StructField('nume', T.StringType(), False),\n",
        "    T.StructField('varsta', T.IntegerType(), False),\n",
        "    T.StructField('ocupatie', T.StringType(), False),\n",
        "    T.StructField('vechime',T.IntegerType(),True),\n",
        "    T.StructField('inactiv', T.BooleanType(), True),\n",
        "    T.StructField('extra', T.ArrayType(T.StringType()), True)\n",
        "])"
      ],
      "metadata": {
        "id": "BjyEH_-_Psqo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dacă se furnizează la citire schema, Spark va ignora sau pune NULL pe coloanele care nu se potrivesc sau nu există:"
      ],
      "metadata": {
        "id": "EEzj8ukrRq1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = spark.createDataFrame(data, schema=data_schema)\n",
        "# or\n",
        "# data_df = spark.read.format('json').schema(data_schema).load('/path/to/folder/or/file')"
      ],
      "metadata": {
        "id": "vDBYWiyWRrsp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Frame – Schema Datelor\n",
        "\n",
        "Fie prin detectare automată, fie prin furnizarea la citire, fiecare Data Frame va avea întotdeauna o structură a\n",
        "datelor foarte bine definită. Pentru fiecare coloană avem numele, tipul de dată și dacă se permit valori de ”NULL”."
      ],
      "metadata": {
        "id": "kIMiCC2sSU1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "➢ Pentru a afișa structură a datelor / schema datelor pentru un Data Frame, folosim:"
      ],
      "metadata": {
        "id": "q5-f3qpLUhwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPu5wkzESW39",
        "outputId": "92b7aef0-c6a8-49b3-eb14-0b1d58e36073"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- nume: string (nullable = false)\n",
            " |-- varsta: integer (nullable = false)\n",
            " |-- ocupatie: string (nullable = false)\n",
            " |-- vechime: integer (nullable = true)\n",
            " |-- inactiv: boolean (nullable = true)\n",
            " |-- extra: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBS: Dacă avem de lucrat cu date care nu au o schemă bine definită, fie date nestructurate, fie date semi-\n",
        "structurate cu multe abateri, atunci NU folosim Data Frame-ul din Spark SQL."
      ],
      "metadata": {
        "id": "RBuMemjtSmcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Frame – Afișarea Datelor\n",
        "\n",
        "De multe ori ne găsim în situația în care este nevoie să depanăm procesul de transformare a datelor. Așadar, Spark\n",
        "ofertă metode pentru acest scop.\n"
      ],
      "metadata": {
        "id": "zN9GgzilS3we"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "➢ Pentru a printa datele la consolă, folosim:"
      ],
      "metadata": {
        "id": "Lz8oW8YKUk7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.show()\n",
        "data_df_json.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIbx_3caS99h",
        "outputId": "516af453-05ec-4c81-98fb-a481aaa7b742"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+-----------+-------+-------+-------------------+\n",
            "|nume|varsta|   ocupatie|vechime|inactiv|              extra|\n",
            "+----+------+-----------+-------+-------+-------------------+\n",
            "|Vali|    23|Programator|      4|   NULL|[Sport, Boardgames]|\n",
            "|Vlad|    34| Instalator|     11|   NULL|         [Alergare]|\n",
            "| Bea|    29|   Reporter|      7|   true|               NULL|\n",
            "+----+------+-----------+-------+-------+-------------------+\n",
            "\n",
            "+--------------------+-------+-----------+--------------------+------+-------+\n",
            "|               extra|inactiv|       nume|            ocupatie|varsta|vechime|\n",
            "+--------------------+-------+-----------+--------------------+------+-------+\n",
            "|            [PV, EV]|   NULL|     Andrei|Specialist marketing|    38|     13|\n",
            "|   [3D Printer, WII]|   NULL|  Alexandru|      Specialist HR |    34|      8|\n",
            "| [AC, EV, 5G Router]|   NULL|     Adrian|       Inginer civil|    45|     23|\n",
            "|              [XBOX]|   NULL|      Alin |   Vânzător  retail |    26|      2|\n",
            "|[5G Router, 3D Pr...|   NULL|      Anton|     Manager proiect|    40|     15|\n",
            "|            [EV, AC]|   NULL|        Ana|  Muncitor alimentar|    35|      9|\n",
            "|[PC, 3D Printer, AC]|  false|     Bogdan|           Farmacist|    50|     32|\n",
            "|                NULL|   true|  Cătălin  |        Medic primar|    20|      0|\n",
            "|      [AC, XBOX, PV]|   NULL|     Cosmin|           Farmacist|    49|     31|\n",
            "|[XBOX, 3D Printer...|  false|   Cristian|    Asistent  social|    44|     21|\n",
            "|[3D Printer, 3D P...|   NULL|    Gabriel|     Asistent social|    44|     21|\n",
            "|[3D Printer, AC, ...|  false|     George|     Agent imobiliar|    46|     25|\n",
            "|   [WII, 3D Printer]|   NULL|   Gheorghe|      Specialist HR |    34|      8|\n",
            "|[WII, 5G Router, ...|   NULL|    Grigore|    Asistent  social|    44|     21|\n",
            "|     [5G Router, PV]|   NULL|      Horia|           Arhitect |    42|     18|\n",
            "|                [PC]|   NULL|      Ilie |     Vânzător retail|    26|      2|\n",
            "|              [XBOX]|   NULL|       Ion |     Vânzător retail|    27|      2|\n",
            "|[EV, XBOX, 3D Pri...|   NULL|      Ionel|     Inginer  civil |    45|     23|\n",
            "|                NULL|   NULL|     Iosif |   Vânzător  retail |    26|      2|\n",
            "|      [PV, XBOX, AC]|  false|     Lucian|     Designer grafic|    49|     30|\n",
            "+--------------------+-------+-----------+--------------------+------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Atenție!**\n",
        "\n",
        "Datele sunt transferate mai întâi de la executori pe Spark Driver înainte de afișare, așadar această\n",
        "metodă limitează numărul de date afișate pentru a evita prăbușirea mașinii principale."
      ],
      "metadata": {
        "id": "2ifYPWpjTUfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Frame – Colectarea Datelor\n",
        "\n",
        "Alte metode pe care Spark le ofertă, nu numai pentru depănarea datelor, dar și pentru rarele situații când am redus datele și vrem să prelucrăm cu alte librării, sunt cele de colectare a datelor în obiecte de Python pe mașina Master."
      ],
      "metadata": {
        "id": "kDdyMuo_TzZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "➢ Pentru a colecta datele într-o listă de Python, folosim:"
      ],
      "metadata": {
        "id": "0sM4HNnrT_oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = data_df.collect()"
      ],
      "metadata": {
        "id": "_q-EsvP9UAet"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "➢ Pentru a colecta datele într-un obiect de Data Frame, dar a librăriei Pandas, folosim:"
      ],
      "metadata": {
        "id": "GKSptjGfUGuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_pandas_pdf = data_df.toPandas()"
      ],
      "metadata": {
        "id": "YaTMaiLJUIcr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Atenție!**\n",
        "\n",
        "Datele sunt transferate mai întâi de la executori pe Spark Driver la colectare. Dacă sunt prea multe\n",
        "date, există pericolul ca procesul de Spark Driver să nu mai facă față și să fie terminat de către sistem."
      ],
      "metadata": {
        "id": "rmr_11wBULUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrierea Datelor\n",
        "\n",
        "În Big Data, după procesare, datele sunt stocate extern în unul sau mai multe formate. PySpark oferă metode pentru scrierea celor mai populare și des utilizare formate."
      ],
      "metadata": {
        "id": "70UMLW2xU4FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "➢ Scrierea fișierelor de tip Parquet."
      ],
      "metadata": {
        "id": "jcjFJKnDVFKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.write.format('parquet').save('/path/to/save/folder')"
      ],
      "metadata": {
        "id": "kkcCes3sVGwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "➢ Scrierea fișierelor de tip JSON Lines.\n"
      ],
      "metadata": {
        "id": "4y9ndtOTU-CT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.write.format('json').save('/path/to/save/folder/')"
      ],
      "metadata": {
        "id": "iDBj-6O7U_rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putem schimba și setările implicite de citire. De exemplu, implicit, Spark nu scrie Header-ul în fișiere CSV.\n",
        "\n",
        "➢ Scrierea fișierelor de tip CSV cu rând de Header."
      ],
      "metadata": {
        "id": "SN7FnFLLVrk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.write.format('csv').option('header', 'true').save('/path/to/save/folder')"
      ],
      "metadata": {
        "id": "7u6bvNuoVuKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBS: Dacă avem mai multe setări, adăugăm la lanț câte un apel la funcția option pentru fiecare setare. O listă cu toate opțiunile disponibile pentru fiecare format găsiți la https://spark.apache.org/docs/latest/sql-data-sources.html."
      ],
      "metadata": {
        "id": "Lh_XOdglVxvy"
      }
    }
  ]
}