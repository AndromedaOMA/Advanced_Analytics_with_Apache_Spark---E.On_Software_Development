{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3BSdg1cTDzkTg5VDSR9zI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndromedaOMA/Advanced_Analytics_with_Apache_Spark---E.On_Software_Development/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriere proiect"
      ],
      "metadata": {
        "id": "tZeome76utxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acest proiect are ca scop principal analiza consumului de energie al unui grup de consumatori fictivi, pe parcursul unui an, a unei companii de energie, folosind tehnicile de analiză din motorul Apache Spark.\n",
        "\n",
        "1. Primul set de date reflectă atât consumul total de energie, cât și detalii specifice, dacă există, despre producția din panouri solare, consumul pentru vehicule electrice (EV), energia furnizată înapoi către rețeaua electrică, consumul și încărcarea bateriilor.\n",
        "2. Al doilea set de date oferă atât tariful pe an și prețurile per kWh în diferite intervale de timp, unice pentru fiecare client în parte, cât și prețul de vânzare la nivelul companiei de energie pe diferite intervale de timp.\n",
        "\n",
        "Proiectul implică curățarea și prelucrarea datelor, completarea valorilor lipsă, iar la final calculul facturii de energie, bonus fiind compararea facturii cu alți clienți similari.\n",
        "\n"
      ],
      "metadata": {
        "id": "VF4AFtmMvGn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Seturile de date"
      ],
      "metadata": {
        "id": "LcEJ47LQvJkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pregătire mediu de lucru"
      ],
      "metadata": {
        "id": "K31gZUBzvpwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "faYdKEGtvsHD",
        "outputId": "5afdc1b1-24b8-4517-cba0-0a64faa1a36a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://dlcdn.apache.org/spark/\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.4.4/spark-3.4.4-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.4-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.4-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as f\n",
        "spark= SparkSession.builder.getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "id": "H2TC5acRvxIE",
        "outputId": "d6ff39d5-5692-47cd-9e00-850ce434ad14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [1 InRelease 14.2 kB/129\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,104 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,837 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,696 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,845 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,266 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,150 kB]\n",
            "Fetched 26.2 MB in 3s (10.4 MB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ab9b67989d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4f39a33b8e80:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setul de Date – Raw Time Series\n",
        "\n",
        "Acest set de date conține informații despre consumul de date a unor clienți fictivi ai unei companii de energie. Structura datelor este una similară cu schemele folosite la momentul actual pentru astfel de date."
      ],
      "metadata": {
        "id": "51NcIq2BvWj2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPvRNKDT-D_f",
        "outputId": "f4103fd4-9707-4b89-969a-9890ae34ca34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+--------------------+------------+--------------------------+\n",
            "|contract_id        |timestamp          |value               |value_source|annotations               |\n",
            "+-------------------+-------------------+--------------------+------------+--------------------------+\n",
            "|04_02_111 _ CHR12  |2023-01-01 06:00:00|0.02591860654732236 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04 _02_111 _CHR12  |2023-01-01 17:00:00|0.07385444264936832 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111 _ CHR12  |2023-01-01 17:30:22|0.08180149515221906 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04 _02_111 _CHR12  |2023-01-01 21:30:00|0.08670661371854547 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04 _ 02 _111_CHR12 |2023-01-02 00:30:00|0.03597601881331959 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04 _02_111 _ CHR12 |2023-01-02 05:30:00|0.03638379308965683 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04 _02 _111 _CHR12 |2023-01-03 10:45:00|0.931575            |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04 _02_111 _CHR12  |2023-01-03 18:30:11|0.08816273670606922 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04 _02_111 _CHR12  |2023-01-04 06:30:04|NULL                |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111 _ CHR12  |2023-01-04 08:00:00|0.01782329751018486 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02 _111 _ CHR12 |2023-01-04 14:45:00|0.06185             |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02 _111 _ CHR12 |2023-01-05 04:30:00|0.03886180041449962 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111 _ CHR12  |2023-01-05 08:00:00|0.02966489301335042 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02 _111 _ CHR12 |2023-01-05 10:30:00|0.0899              |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04 _02_111 _ CHR12 |2023-01-07 09:15:00|0.013017644707343774|measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111 _ CHR12  |2023-01-07 12:15:00|0.253075            |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04 _02_111 _CHR12  |2023-01-07 16:45:00|0.10331219017612531 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111 _ CHR12  |2023-01-07 23:30:00|0.07445077454380414 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111 _ CHR12  |2023-01-08 09:30:00|0.01659186356495865 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04 _02_111 _ CHR12 |2023-01-08 11:00:00|0.09935000000000001 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "+-------------------+-------------------+--------------------+------------+--------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parquet_path = '/content/drive/MyDrive/E.on/E.on_Data/Data/Project/raw_time_series/parquet'\n",
        "raw_time_series_df = spark.read.parquet(parquet_path)\n",
        "raw_time_series_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setul de Date – Customer Tariff\n",
        "\n",
        "Acest set de date conține informații despre tarifele și prețurile a unor clienți fictivi, într-un interval de timp, ai unei companii de energie. Structura este una similară cu cea folosite la momentul actual pentru astfel de date."
      ],
      "metadata": {
        "id": "7gIQ6kOpwRTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_path = '/content/drive/MyDrive/E.on/E.on_Data/Data/Project/customer_tariff/parquet'\n",
        "customer_tariff_df = spark.read.parquet(parquet_path)\n",
        "customer_tariff_df.show()"
      ],
      "metadata": {
        "id": "_gI1Y7QmwVyr",
        "outputId": "9c2366da-f4cc-44be-8356-d6512bc12b32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----------------------------+--------------------------+-----------------+-----------+------+\n",
            "|    contract_id|target_local_start_timestamp|target_local_end_timestamp|      tariff_name|charge_type| price|\n",
            "+---------------+----------------------------+--------------------------+-----------------+-----------+------+\n",
            "|04_02_111_CHR28|         2022-12-01 00:00:00|       2023-02-16 00:00:00|     Electric Pro|        buy| 0.302|\n",
            "|04_02_111_CHR28|         2023-02-16 00:00:00|       2024-05-19 00:00:00| Electric Loyalty|        buy|0.3938|\n",
            "|04_02_111_CHR28|         2024-05-19 00:00:00|       2024-08-31 00:00:00|     Eco Electric|        buy|0.2095|\n",
            "|04_02_111_CHR28|         2024-08-31 00:00:00|       2024-10-09 00:00:00| Electric Loyalty|        buy|0.4047|\n",
            "|04_02_111_CHR28|         2024-10-09 00:00:00|       2024-11-09 00:00:00|Business Electric|        buy|0.1716|\n",
            "|04_02_111_CHR28|         2024-11-09 00:00:00|       2024-12-11 00:00:00|     Electric Pro|        buy|0.2944|\n",
            "|04_02_111_CHR28|         2024-12-11 00:00:00|       2025-01-01 00:00:00| Digital Electric|        buy|0.1781|\n",
            "|04_02_111_CHR28|         2022-12-01 00:00:00|       2023-03-15 00:00:00|           SState|       sell|0.0842|\n",
            "|04_02_111_CHR28|         2023-03-15 00:00:00|       2024-08-25 00:00:00|         EctoGrid|       sell| 0.069|\n",
            "|04_02_111_CHR28|         2024-08-25 00:00:00|       2025-01-01 00:00:00|            OpCom|       sell|0.1182|\n",
            "|04_02_111_CHR05|         2022-12-01 00:00:00|       2023-02-12 00:00:00|   Electric Optim|        buy|0.2277|\n",
            "|04_02_111_CHR05|         2023-02-12 00:00:00|       2024-05-01 00:00:00|   Electric Optim|        buy| 0.243|\n",
            "|04_02_111_CHR05|         2024-05-01 00:00:00|       2024-08-10 00:00:00| Digital Electric|        buy|0.2318|\n",
            "|04_02_111_CHR05|         2024-08-10 00:00:00|       2024-09-18 00:00:00|Business Electric|        buy| 0.379|\n",
            "|04_02_111_CHR05|         2024-09-18 00:00:00|       2024-10-18 00:00:00|Business Electric|        buy|0.1613|\n",
            "|04_02_111_CHR05|         2024-10-18 00:00:00|       2024-11-19 00:00:00|   Electric Optim|        buy|0.2669|\n",
            "|04_02_111_CHR05|         2024-11-19 00:00:00|       2024-12-09 00:00:00|Business Electric|        buy|0.2148|\n",
            "|04_02_111_CHR05|         2024-12-09 00:00:00|       2025-01-01 00:00:00|     Electric Pro|        buy|0.2168|\n",
            "|04_02_111_CHR05|         2022-12-01 00:00:00|       2023-02-16 00:00:00|         EctoGrid|       sell| 0.082|\n",
            "|04_02_111_CHR05|         2023-02-16 00:00:00|       2024-05-19 00:00:00|         EctoGrid|       sell|0.0993|\n",
            "+---------------+----------------------------+--------------------------+-----------------+-----------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Cerințe"
      ],
      "metadata": {
        "id": "sVoOj1Lywlmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curățarea Datelor\n",
        "\n",
        "Setul de date al consumului prezintă unele probleme, precum spații suplimentare și informații lipsă sau semi-structurate."
      ],
      "metadata": {
        "id": "NL9UXtiEwpFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acestea trebuie rectificate înainte de a trece mai departe:\n",
        "\n",
        "1. Curățarea coloanei „contract_id” de spații suplimentare\n",
        "2. Setarea coloanei „value_source” cu valoarea „missing” atunci când valoarea lipsește.\n",
        "3. Coloana „timestamp” are variații mici ce trebuie rectificate. Contoarele măsoară consumul odată la 15 minute exact începând cu ora 00:00, dar din cauza procesării, pot apărea variații la timpul pe care îl trimit. (Indiciu: Se găsește cea mai apropriat multiplu de 15 la minute și se scot secundele)\n",
        "\n"
      ],
      "metadata": {
        "id": "qpdaqc2qwvIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\n",
        "raw_time_series_df = raw_time_series_df.withColumn('contract_id', f.translate(f.col('contract_id'), ' ', ''))\n",
        "raw_time_series_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6wxGY1nsWCy",
        "outputId": "d7aeb271-7a90-4103-aee6-815181bd9f93"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------------+--------------------+------------+--------------------------+\n",
            "|contract_id    |timestamp          |value               |value_source|annotations               |\n",
            "+---------------+-------------------+--------------------+------------+--------------------------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-01 17:30:22|0.08180149515221906 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-02 05:30:00|0.03638379308965683 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-03 10:45:00|0.931575            |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-03 18:30:11|0.08816273670606922 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-04 06:30:04|NULL                |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-04 08:00:00|0.01782329751018486 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-04 14:45:00|0.06185             |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-05 04:30:00|0.03886180041449962 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-05 08:00:00|0.02966489301335042 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-05 10:30:00|0.0899              |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 09:15:00|0.013017644707343774|measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 12:15:00|0.253075            |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 16:45:00|0.10331219017612531 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 23:30:00|0.07445077454380414 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-08 09:30:00|0.01659186356495865 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-08 11:00:00|0.09935000000000001 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "+---------------+-------------------+--------------------+------------+--------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.\n",
        "# raw_time_series_df = raw_time_series_df.withColumn('value_source', f.coalesce(f.col('value_source'), f.lit('missing')))\n",
        "raw_time_series_df = raw_time_series_df.withColumn('value_source', f.when(f.col('value').isNull(), f.lit('missing')).otherwise(f.col('value_source')))\n",
        "raw_time_series_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBoMxA4Qubys",
        "outputId": "3cc67bd6-9df2-4d20-a126-157985e71af9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------------+--------------------+------------+--------------------------+\n",
            "|contract_id    |timestamp          |value               |value_source|annotations               |\n",
            "+---------------+-------------------+--------------------+------------+--------------------------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-01 17:30:22|0.08180149515221906 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-02 05:30:00|0.03638379308965683 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-03 10:45:00|0.931575            |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-03 18:30:11|0.08816273670606922 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-04 06:30:04|NULL                |missing     |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-04 08:00:00|0.01782329751018486 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-04 14:45:00|0.06185             |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-05 04:30:00|0.03886180041449962 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-05 08:00:00|0.02966489301335042 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-05 10:30:00|0.0899              |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 09:15:00|0.013017644707343774|measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 12:15:00|0.253075            |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 16:45:00|0.10331219017612531 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 23:30:00|0.07445077454380414 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-08 09:30:00|0.01659186356495865 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-08 11:00:00|0.09935000000000001 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "+---------------+-------------------+--------------------+------------+--------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.\n",
        "raw_time_series_df = raw_time_series_df.withColumn('timestamp',\n",
        "                                                   f.from_unixtime((f.unix_timestamp('timestamp') / 900).cast('int') * 900).cast('timestamp'))\n",
        "raw_time_series_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9_aYKjYwbgv",
        "outputId": "9645b266-e1e9-497e-d8af-d5f9d4dca1fb"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------------+--------------------+------------+--------------------------+\n",
            "|contract_id    |timestamp          |value               |value_source|annotations               |\n",
            "+---------------+-------------------+--------------------+------------+--------------------------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-01 17:30:00|0.08180149515221906 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-02 05:30:00|0.03638379308965683 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-03 10:45:00|0.931575            |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-03 18:30:00|0.08816273670606922 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-04 06:30:00|NULL                |missing     |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-04 08:00:00|0.01782329751018486 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-04 14:45:00|0.06185             |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-05 04:30:00|0.03886180041449962 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-05 08:00:00|0.02966489301335042 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-05 10:30:00|0.0899              |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 09:15:00|0.013017644707343774|measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 12:15:00|0.253075            |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 16:45:00|0.10331219017612531 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-07 23:30:00|0.07445077454380414 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-08 09:30:00|0.01659186356495865 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "|04_02_111_CHR12|2023-01-08 11:00:00|0.09935000000000001 |measurement |{\"region\":\"Europe/Berlin\"}|\n",
            "+---------------+-------------------+--------------------+------------+--------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extragerea Informațiilor de localizare și filtrarea datelor invalide\n",
        "\n",
        "Setul de date al consumului prezintă unele probleme, precum spații suplimentare și informații lipsă sau semi-structurate."
      ],
      "metadata": {
        "id": "TeGEPAHEw8vO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acestea trebuie rectificate înainte de a trece mai departe:\n",
        "\n",
        "1. Extragerea unei noi coloane „region” din „annotations” (Se recomandă folosirea funcțiilor de Spark de procesare JSON)\n",
        "2. Clienții cu regiuni invalide se vor scoate din setul de date și se vor salva pe disk într-o locație separată.\n",
        "3. Extragerea datei din coloana „timestamp” într-o nouă coloană „utc_date”\n",
        "4. Calcularea datei locale pentru data și ora din „timestamp”, pe baza regiunii, într-o nouă coloană „local_timestamp”\n",
        "5. Extragerea datei din coloana „ local_timestamp” într-o nouă coloană „local_date”\n",
        "\n"
      ],
      "metadata": {
        "id": "U5-RSyFFxBAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\n",
        "raw_time_series_df = raw_time_series_df.withColumn('region', f.get_json_object(f.col('annotations'), '$.region'))\n",
        "print(raw_time_series_df.count())\n",
        "raw_time_series_df.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NecepvEh-_3p",
        "outputId": "14337c70-02e4-488d-ccd6-420f207404dc"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3549244\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+\n",
            "|contract_id    |timestamp          |value              |value_source|annotations               |region       |\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|\n",
            "|04_02_111_CHR12|2023-01-01 17:30:00|0.08180149515221906|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.\n",
        "# Am verificat câte regiuni se găsesc în setul nostru de date -> REZULTAT: Europe/Berlin și WakaWaka\n",
        "view_regions = raw_time_series_df.select('region').distinct().collect()\n",
        "print(view_regions)\n",
        "\n",
        "test_raw_time_series_df = raw_time_series_df.groupBy('region').avg()\n",
        "test_raw_time_series_df.show(truncate=False)\n",
        "\n",
        "# Așadar:\n",
        "out_path = '/content/drive/MyDrive/E.on/E.on_Data/Data/Project/outputs'\n",
        "invalid_regions_df = raw_time_series_df.where(f.col('region')!='Europe/Berlin')\n",
        "invalid_regions_df.write.mode('overwrite').format('parquet').save(out_path)\n",
        "\n",
        "# print(f\"no. of rows of raw_time_series_df: {raw_time_series_df.count()}\")\n",
        "raw_time_series_df.show(5, truncate=False)\n",
        "\n",
        "# print(f\"no. of rows of invalid_regions_df: {invalid_regions_df.count()}\")\n",
        "invalid_regions_df.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjzAlJMcM_cV",
        "outputId": "6f5c71d4-1169-4e56-d147-78219d91cce5"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(region='Europe/Berlin'), Row(region='WakaWaka')]\n",
            "+-------------+-------------------+\n",
            "|region       |avg(value)         |\n",
            "+-------------+-------------------+\n",
            "|Europe/Berlin|0.490498974431521  |\n",
            "|WakaWaka     |0.13178303122889254|\n",
            "+-------------+-------------------+\n",
            "\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+\n",
            "|contract_id    |timestamp          |value              |value_source|annotations               |region       |\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|\n",
            "|04_02_111_CHR12|2023-01-01 17:30:00|0.08180149515221906|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------+-------------------+-------------------+------------+---------------------+--------+\n",
            "|contract_id    |timestamp          |value              |value_source|annotations          |region  |\n",
            "+---------------+-------------------+-------------------+------------+---------------------+--------+\n",
            "|01_02_155_CHR98|2023-01-01 00:15:00|0.24727942049503326|measurement |{\"region\":\"WakaWaka\"}|WakaWaka|\n",
            "|01_02_155_CHR98|2023-01-01 00:45:00|NULL               |missing     |{\"region\":\"WakaWaka\"}|WakaWaka|\n",
            "|01_02_155_CHR98|2023-01-01 01:30:00|0.09512702375650406|measurement |{\"region\":\"WakaWaka\"}|WakaWaka|\n",
            "|01_02_155_CHR98|2023-01-01 02:00:00|0.26903796195983887|measurement |{\"region\":\"WakaWaka\"}|WakaWaka|\n",
            "|01_02_155_CHR98|2023-01-01 03:45:00|0.10639405995607376|measurement |{\"region\":\"WakaWaka\"}|WakaWaka|\n",
            "+---------------+-------------------+-------------------+------------+---------------------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.\n",
        "raw_time_series_df = raw_time_series_df.withColumn('utc_date', f.to_date(f.col('timestamp')))\n",
        "raw_time_series_df.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "YolB0YibWIia",
        "outputId": "2e58a9d7-fc68-4abd-f85b-a397822d371e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+\n",
            "|contract_id    |timestamp          |value              |value_source|annotations               |region       |utc_date  |\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-01 17:30:00|0.08180149515221906|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-02|\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.\n",
        "raw_time_series_df = raw_time_series_df.withColumn('local_timestamp', f.from_utc_timestamp(f.col('timestamp'), f.col('region')))\n",
        "raw_time_series_df.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "knVmJWBNiHGm",
        "outputId": "2d723bac-94bc-4d65-c562-3a4d5e7b6d72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+\n",
            "|contract_id    |timestamp          |value              |value_source|annotations               |region       |utc_date  |local_timestamp    |\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 07:00:00|\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:00:00|\n",
            "|04_02_111_CHR12|2023-01-01 17:30:00|0.08180149515221906|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:30:00|\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 22:30:00|\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-02|2023-01-02 01:30:00|\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.\n",
        "raw_time_series_df = raw_time_series_df.withColumn('local_date', f.to_date('local_timestamp'))\n",
        "raw_time_series_df.show(5, truncate=False)"
      ],
      "metadata": {
        "id": "3wTe5jlnubHK",
        "outputId": "3cb31983-748a-4815-ef41-da3034b047cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+\n",
            "|contract_id    |timestamp          |value              |value_source|annotations               |region       |utc_date  |local_timestamp    |local_date|\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 07:00:00|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:00:00|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-01 17:30:00|0.08180149515221906|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:30:00|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 22:30:00|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-02|2023-01-02 01:30:00|2023-01-02|\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extragerea Informațiilor de consum\n",
        "\n",
        "Setul de date al consumului prezintă unele probleme, precum spații suplimentare și informații lipsă sau semi-structurate.\n",
        "\n"
      ],
      "metadata": {
        "id": "piuQbNfhxZfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acestea trebuie rectificate înainte de a trece mai departe:\n",
        "\n",
        "1. Extragerea din coloana „annotations” a consumului de vehicul electric (EV), baterie (BATTERY_IN) și consumul trimis spre rețeaua electrică (GRID_SELL) în coloanele „sent_to_ev”, „sent_to_battery” și „sent_to_grid”. În cazul în care valoare lipsește, se consideră consumul 0.\n",
        "2. Extragerea din coloana „annotations” a energiei primite de la panourile solare (PV) și baterie (BATTERY_OUT) în coloanele „received_from_pv” și „received_from_battery”. În cazul în care valoare lipsește, se consideră energia primită 0."
      ],
      "metadata": {
        "id": "TTPlboObsE6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Seturile de date disponibile:\")\n",
        "print(\"raw_time_series_df\")\n",
        "raw_time_series_df.show(5, truncate=False)\n",
        "print(\"customer_tariff_df\")\n",
        "customer_tariff_df.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqlZiuOXrOIa",
        "outputId": "6b93651f-7c98-409f-b4d7-e7ef2695135a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seturile de date disponibile:\n",
            "raw_time_series_df\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+\n",
            "|contract_id    |timestamp          |value              |value_source|annotations               |region       |utc_date  |local_timestamp    |local_date|\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 07:00:00|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:00:00|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-01 17:30:00|0.08180149515221906|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:30:00|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 22:30:00|2023-01-01|\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-02|2023-01-02 01:30:00|2023-01-02|\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "customer_tariff_df\n",
            "+---------------+----------------------------+--------------------------+-----------------+-----------+------+\n",
            "|contract_id    |target_local_start_timestamp|target_local_end_timestamp|tariff_name      |charge_type|price |\n",
            "+---------------+----------------------------+--------------------------+-----------------+-----------+------+\n",
            "|04_02_111_CHR28|2022-12-01 00:00:00         |2023-02-16 00:00:00       |Electric Pro     |buy        |0.302 |\n",
            "|04_02_111_CHR28|2023-02-16 00:00:00         |2024-05-19 00:00:00       |Electric Loyalty |buy        |0.3938|\n",
            "|04_02_111_CHR28|2024-05-19 00:00:00         |2024-08-31 00:00:00       |Eco Electric     |buy        |0.2095|\n",
            "|04_02_111_CHR28|2024-08-31 00:00:00         |2024-10-09 00:00:00       |Electric Loyalty |buy        |0.4047|\n",
            "|04_02_111_CHR28|2024-10-09 00:00:00         |2024-11-09 00:00:00       |Business Electric|buy        |0.1716|\n",
            "+---------------+----------------------------+--------------------------+-----------------+-----------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\n",
        "\n",
        "# Verificăm tipul de date a coloanei \"annotations\":\n",
        "print(\"Tipurile valorilot coloanei annotations:\")\n",
        "raw_time_series_df.select('annotations').printSchema()\n",
        "# Verificăm valorile distincte pe care le primește coloana \"annotations\":\n",
        "print(\"Valorile distincte ale coloanei annotations:\")\n",
        "raw_time_series_df.select(('annotations')).distinct().show(5, truncate=False)\n",
        "print('===' * 29)\n",
        "\n",
        "raw_time_series_df = raw_time_series_df.withColumn('sent_to_ev', f.when(f.get_json_object(f.col('annotations'), '$.events.EV').isNotNull(), f.get_json_object(f.col('annotations'), '$.events.EV')).otherwise(0))\n",
        "raw_time_series_df = raw_time_series_df.withColumn('sent_to_battery', f.when(f.get_json_object(f.col('annotations'), '$.events.BATTERY_IN').isNotNull(), f.get_json_object(f.col('annotations'), '$.events.BATTERY_IN')).otherwise(0))\n",
        "raw_time_series_df = raw_time_series_df.withColumn('sent_to_grid', f.when(f.get_json_object(f.col('annotations'), '$.events.GRID_SELL').isNotNull(), f.get_json_object(f.col('annotations'), '$.events.GRID_SELL')).otherwise(0))\n",
        "raw_time_series_df.show(5,truncate=False)\n",
        "\n",
        "# Pentru validarea faptului că valorile consumurilor nu sunt numai nule\n",
        "raw_time_series_df.select('sent_to_ev').distinct().show(5,truncate=False)\n",
        "raw_time_series_df.select('sent_to_battery').distinct().show(5,truncate=False)\n",
        "raw_time_series_df.select('sent_to_grid').distinct().show(5,truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7_cb9Pbsl-M",
        "outputId": "3ae7af14-c250-4f76-e8e6-025b0b47cbae"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tipurile valorilot coloanei annotations:\n",
            "root\n",
            " |-- annotations: string (nullable = true)\n",
            "\n",
            "Valorile distincte ale coloanei annotations:\n",
            "+-------------------------------------------------------------------------------------+\n",
            "|annotations                                                                          |\n",
            "+-------------------------------------------------------------------------------------+\n",
            "|{\"region\":\"Europe/Berlin\",\"events\":{\"GRID_SELL\":\"1.753088873001001\",\"PV\":\"1.839175\"}}|\n",
            "|{\"region\":\"Europe/Berlin\",\"events\":{\"EV\":\"1.035\",\"PV\":\"1.674475\"}}                   |\n",
            "|{\"region\":\"Europe/Berlin\",\"events\":{\"PV\":\"1.076\"}}                                   |\n",
            "|{\"region\":\"Europe/Berlin\",\"events\":{\"PV\":\"1.077475\"}}                                |\n",
            "|{\"region\":\"Europe/Berlin\",\"events\":{\"BATTERY_IN\":\"1.633989566467353\",\"PV\":\"1.69435\"}}|\n",
            "+-------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "=======================================================================================\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+----------+---------------+------------+\n",
            "|contract_id    |timestamp          |value              |value_source|annotations               |region       |utc_date  |local_timestamp    |local_date|sent_to_ev|sent_to_battery|sent_to_grid|\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+----------+---------------+------------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 07:00:00|2023-01-01|0         |0              |0           |\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:00:00|2023-01-01|0         |0              |0           |\n",
            "|04_02_111_CHR12|2023-01-01 17:30:00|0.08180149515221906|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:30:00|2023-01-01|0         |0              |0           |\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 22:30:00|2023-01-01|0         |0              |0           |\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-02|2023-01-02 01:30:00|2023-01-02|0         |0              |0           |\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+----------+---------------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------+\n",
            "|sent_to_ev       |\n",
            "+-----------------+\n",
            "|1.035009824399999|\n",
            "|2.472000234815109|\n",
            "|2.36729999999999 |\n",
            "|1.334150000133214|\n",
            "|2.38010024675264 |\n",
            "+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------+\n",
            "|sent_to_battery  |\n",
            "+-----------------+\n",
            "|1.073343537998281|\n",
            "|1.615363012036317|\n",
            "|1.298782850553927|\n",
            "|1.382732743659848|\n",
            "|1.473304377044588|\n",
            "+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------+\n",
            "|sent_to_grid     |\n",
            "+-----------------+\n",
            "|1.792016403175495|\n",
            "|1.345920470827863|\n",
            "|1.435150673564946|\n",
            "|1.155372549078849|\n",
            "|1.411143449072801|\n",
            "+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.\n",
        "raw_time_series_df = raw_time_series_df.withColumn('received_from_pv', f.when(f.get_json_object(f.col('annotations'), '$.events.PV').isNotNull(), f.get_json_object(f.col('annotations'), '$.events.PV')).otherwise(0))\n",
        "raw_time_series_df = raw_time_series_df.withColumn('received_from_battery', f.when(f.get_json_object(f.col('annotations'), '$.events.BATTERY_IN').isNotNull(), f.get_json_object(f.col('annotations'), '$.events.PV')).otherwise(0))\n",
        "\n",
        "raw_time_series_df.show(5,truncate=False)\n",
        "raw_time_series_df.select('received_from_pv').distinct().show(5,truncate=False)\n",
        "raw_time_series_df.select('received_from_battery').distinct().show(5,truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttb6TqBID7AA",
        "outputId": "440def7f-37ad-4f60-f661-5c35f5ae43a3"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+----------+---------------+------------+----------------+---------------------+\n",
            "|contract_id    |timestamp          |value              |value_source|annotations               |region       |utc_date  |local_timestamp    |local_date|sent_to_ev|sent_to_battery|sent_to_grid|received_from_pv|received_from_battery|\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+----------+---------------+------------+----------------+---------------------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 07:00:00|2023-01-01|0         |0              |0           |0               |0                    |\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:00:00|2023-01-01|0         |0              |0           |0               |0                    |\n",
            "|04_02_111_CHR12|2023-01-01 17:30:00|0.08180149515221906|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:30:00|2023-01-01|0         |0              |0           |0               |0                    |\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 22:30:00|2023-01-01|0         |0              |0           |0               |0                    |\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-02|2023-01-02 01:30:00|2023-01-02|0         |0              |0           |0               |0                    |\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+----------+---------------+------------+----------------+---------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------------+\n",
            "|received_from_pv|\n",
            "+----------------+\n",
            "|1.15885         |\n",
            "|1.146925        |\n",
            "|1.774175        |\n",
            "|1.731425        |\n",
            "|1.88595         |\n",
            "+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------------------+\n",
            "|received_from_battery|\n",
            "+---------------------+\n",
            "|1.731425             |\n",
            "|1.146925             |\n",
            "|1.774175             |\n",
            "|1.54965              |\n",
            "|1.1022534375         |\n",
            "+---------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtrarea consumului neobișnuit\n",
        "\n",
        "Anumite valori ale consumului sunt neobișnuit de mari și este necesară scoaterea lor."
      ],
      "metadata": {
        "id": "t90SC0hNxkhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.  Setarea coloanei „value_source” în „plausability_check_failed” pentru valorile cu consum neobișnuit din setul de date. (Decizia valorilor mari fie se face cu o analiză vizuală a datelor (sortarea și identificarea lor vizual fie prin agregări simple fie mai complicate) sau bonus, pentru cine dorește, prin tehnici de învățare automată.)\n",
        "2. Salvarea separată a datelor cu „value_source” având valoare „plausability_check_failed” într-o locație separată. Atenție, datele nu se scot din setul de date.\n",
        "3. Setarea coloanei „value” în NULL pentru datele cu „value_source” având valoare „plausability_check_failed”.\n",
        "\n"
      ],
      "metadata": {
        "id": "rK0jyoUrxqBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. + BONUS\n",
        "raw_time_series_df.show(5, truncate=False)\n",
        "\n",
        "# IMPLEMENTAREA I\n",
        "print(\"IMPLEMENTAREA I\")\n",
        "\n",
        "# Prea multă putere de calcul necesară pentru realizarea plot-urilor\n",
        "# pandas_df = view_details_rts_df.toPandas()\n",
        "# pandas_df.value.value_counts().plot(kind='bar')\n",
        "\n",
        "view_details_rts_df = raw_time_series_df.select('value').distinct().orderBy(f.col('value').desc()).limit(2000)\n",
        "print(\"view_details_rts_df: \")\n",
        "view_details_rts_df.show(truncate=False)\n",
        "out_path = '/content/drive/MyDrive/E.on/E.on_Data/Data/Project/outputs/view_details_rts_df'\n",
        "view_details_rts_df.coalesce(1).write.mode('overwrite').csv(out_path)\n",
        "\n",
        "# Din CSV-ul salvat mai sus deducem aspectul constant a creșterii valorilor, dar în intervalul 39-42 valorile încep să crească mai alarmant.\n",
        "raw_time_series_df = raw_time_series_df.withColumn('value_source', f.when(f.col('value')>39, 'plausability_check_failed').otherwise(f.col('value_source')))\n",
        "\n",
        "# IMPLEMENTAREA II (BONUS)\n",
        "print('=====' * 10)\n",
        "print(\"IMPLEMENTAREA II\")\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Corectare coloană 'value' prin ștergerea valorilor de 0\n",
        "raw_time_series_df = raw_time_series_df.where(f.col('value')!=0)\n",
        "\n",
        "# Vectorizare pe coloana 'value'\n",
        "vector_assembler = VectorAssembler(inputCols=['value'], outputCol='value_features')\n",
        "data_vectorized_df = vector_assembler.transform(raw_time_series_df)\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(featuresCol='value_features', predictionCol='cluster', k=2)\n",
        "model = kmeans.fit(data_vectorized_df)\n",
        "\n",
        "# Calculează distanța față de centrul clusterului\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "center = model.clusterCenters()[0]\n",
        "\n",
        "def distance(v):\n",
        "    return float((v - center).norm(2))\n",
        "\n",
        "distance_udf = f.udf(distance, DoubleType())\n",
        "data_with_distance = data_vectorized_df.withColumn(\"distance\", distance_udf(\"value_features\"))\n",
        "\n",
        "# Setează „value” pentru valori anormale\n",
        "threshold = 3.0\n",
        "raw_time_series_df = data_with_distance.withColumn(\n",
        "    \"value_source\",\n",
        "    f.when(f.col(\"distance\") > threshold, \"plausability_check_failed\")\n",
        ")\n",
        "\n",
        "view_value_sources = raw_time_series_df.select('value_source').distinct()\n",
        "view_value_sources.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFF0xkluJABK",
        "outputId": "1e15897c-23d5-4580-b524-f7a28408b396"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+----------+---------------+------------+----------------+---------------------+\n",
            "|contract_id    |timestamp          |value              |value_source|annotations               |region       |utc_date  |local_timestamp    |local_date|sent_to_ev|sent_to_battery|sent_to_grid|received_from_pv|received_from_battery|\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+----------+---------------+------------+----------------+---------------------+\n",
            "|04_02_111_CHR12|2023-01-01 06:00:00|0.02591860654732236|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 07:00:00|2023-01-01|0         |0              |0           |0               |0                    |\n",
            "|04_02_111_CHR12|2023-01-01 17:00:00|0.07385444264936832|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:00:00|2023-01-01|0         |0              |0           |0               |0                    |\n",
            "|04_02_111_CHR12|2023-01-01 17:30:00|0.08180149515221906|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 18:30:00|2023-01-01|0         |0              |0           |0               |0                    |\n",
            "|04_02_111_CHR12|2023-01-01 21:30:00|0.08670661371854547|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-01|2023-01-01 22:30:00|2023-01-01|0         |0              |0           |0               |0                    |\n",
            "|04_02_111_CHR12|2023-01-02 00:30:00|0.03597601881331959|measurement |{\"region\":\"Europe/Berlin\"}|Europe/Berlin|2023-01-02|2023-01-02 01:30:00|2023-01-02|0         |0              |0           |0               |0                    |\n",
            "+---------------+-------------------+-------------------+------------+--------------------------+-------------+----------+-------------------+----------+----------+---------------+------------+----------------+---------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "IMPLEMENTAREA I\n",
            "view_details_rts_df: \n",
            "+-----------------+\n",
            "|value            |\n",
            "+-----------------+\n",
            "|83.7633960367971 |\n",
            "|83.45165458104191|\n",
            "|80.5203908045932 |\n",
            "|77.88889162124525|\n",
            "|77.48032877642238|\n",
            "|76.88911895599986|\n",
            "|76.69798782063941|\n",
            "|75.76717163660166|\n",
            "|75.63363148646712|\n",
            "|75.1682792514436 |\n",
            "|74.9268292510051 |\n",
            "|74.39430262643208|\n",
            "|74.17691963148631|\n",
            "|73.79597194030637|\n",
            "|73.62673236058076|\n",
            "|73.50758279260636|\n",
            "|73.24851498214987|\n",
            "|73.1949246302417 |\n",
            "|72.90555560294808|\n",
            "|72.73071224721161|\n",
            "+-----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "==================================================\n",
            "IMPLEMENTAREA II\n",
            "+-------------------------+\n",
            "|value_source             |\n",
            "+-------------------------+\n",
            "|plausability_check_failed|\n",
            "|NULL                     |\n",
            "+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.\n",
        "out_path = '/content/drive/MyDrive/E.on/E.on_Data/Data/Project/outputs/plausability_check_failed'\n",
        "raw_time_series_df.where(f.col('value_source')=='plausability_check_failed').write.mode('overwrite').format('parquet').save(out_path)"
      ],
      "metadata": {
        "id": "pKHP6lvdHHq3",
        "outputId": "9ca33c64-134c-4b2a-9929-9305f3579a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o2110.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 656.0 failed 1 times, most recent failure: Lost task 1.0 in stage 656.0 (TID 956) (4f39a33b8e80 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/drive/MyDrive/E.on/E.on_Data/Data/Project/outputs/plausability_check_failed.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.zone.ZoneRulesException: Unknown time-zone ID: WakaWaka\n\tat java.base/java.time.zone.ZoneRulesProvider.getProvider(ZoneRulesProvider.java:279)\n\tat java.base/java.time.zone.ZoneRulesProvider.getRules(ZoneRulesProvider.java:234)\n\tat java.base/java.time.ZoneRegion.ofId(ZoneRegion.java:120)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:408)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:356)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:312)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.getZoneId(SparkDateTimeUtils.scala:46)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.getZoneId$(SparkDateTimeUtils.scala:39)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.getZoneId(DateTimeUtils.scala:40)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.fromUTCTime(DateTimeUtils.scala:496)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils.fromUTCTime(DateTimeUtils.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/drive/MyDrive/E.on/E.on_Data/Data/Project/outputs/plausability_check_failed.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.time.zone.ZoneRulesException: Unknown time-zone ID: WakaWaka\n\tat java.base/java.time.zone.ZoneRulesProvider.getProvider(ZoneRulesProvider.java:279)\n\tat java.base/java.time.zone.ZoneRulesProvider.getRules(ZoneRulesProvider.java:234)\n\tat java.base/java.time.ZoneRegion.ofId(ZoneRegion.java:120)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:408)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:356)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:312)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.getZoneId(SparkDateTimeUtils.scala:46)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.getZoneId$(SparkDateTimeUtils.scala:39)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.getZoneId(DateTimeUtils.scala:40)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.fromUTCTime(DateTimeUtils.scala:496)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils.fromUTCTime(DateTimeUtils.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-243f2f030e41>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mout_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/E.on/E.on_Data/Data/Project/outputs/plausability_check_failed'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mraw_time_series_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value_source'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'plausability_check_failed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2110.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 656.0 failed 1 times, most recent failure: Lost task 1.0 in stage 656.0 (TID 956) (4f39a33b8e80 executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/drive/MyDrive/E.on/E.on_Data/Data/Project/outputs/plausability_check_failed.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.zone.ZoneRulesException: Unknown time-zone ID: WakaWaka\n\tat java.base/java.time.zone.ZoneRulesProvider.getProvider(ZoneRulesProvider.java:279)\n\tat java.base/java.time.zone.ZoneRulesProvider.getRules(ZoneRulesProvider.java:234)\n\tat java.base/java.time.ZoneRegion.ofId(ZoneRegion.java:120)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:408)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:356)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:312)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.getZoneId(SparkDateTimeUtils.scala:46)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.getZoneId$(SparkDateTimeUtils.scala:39)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.getZoneId(DateTimeUtils.scala:40)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.fromUTCTime(DateTimeUtils.scala:496)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils.fromUTCTime(DateTimeUtils.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/drive/MyDrive/E.on/E.on_Data/Data/Project/outputs/plausability_check_failed.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.time.zone.ZoneRulesException: Unknown time-zone ID: WakaWaka\n\tat java.base/java.time.zone.ZoneRulesProvider.getProvider(ZoneRulesProvider.java:279)\n\tat java.base/java.time.zone.ZoneRulesProvider.getRules(ZoneRulesProvider.java:234)\n\tat java.base/java.time.ZoneRegion.ofId(ZoneRegion.java:120)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:408)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:356)\n\tat java.base/java.time.ZoneId.of(ZoneId.java:312)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.getZoneId(SparkDateTimeUtils.scala:46)\n\tat org.apache.spark.sql.catalyst.util.SparkDateTimeUtils.getZoneId$(SparkDateTimeUtils.scala:39)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.getZoneId(DateTimeUtils.scala:40)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils$.fromUTCTime(DateTimeUtils.scala:496)\n\tat org.apache.spark.sql.catalyst.util.DateTimeUtils.fromUTCTime(DateTimeUtils.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.\n",
        "raw_time_series_df = raw_time_series_df.withColumn('value', f.when(f.col('value_source') == 'plausability_check_failed', f.lit('Null')).otherwise(f.col('value')))\n",
        "view_value = raw_time_series_df.select('value').distinct()\n",
        "view_value.show(truncate=False)"
      ],
      "metadata": {
        "id": "SJzu0kn5IMyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Completarea valorilor lipsă\n",
        "\n",
        "Anumiți clienți au lipsuri prezintă câteva lipsuri în consum, unele dintre ele adăugate de noi la pasul precedent:\n"
      ],
      "metadata": {
        "id": "_j9PZi3Dx4lK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Prezicerea valorii „value” atunci când ea este NULL folosind următoarea metodă:\n",
        "\n",
        "  * Calculăm mai întâi media din ultimele 8 săptămâni a valorilor din aceeași zi a săptămânii la aceeași oră, minut și secundă. Se folosesc din ultimele 8 săptămâni doar valorile care nu lipsesc, au coloana „value_source” setată pe valoarea „measurement”.\n",
        "  * Facem suma coloanelor „sent_to_ev”, „sent_to_battery” și „sent_to_grid”.\n",
        "  * Facem suma coloanelor „received_from_pv” și „received_from_battery”.\n",
        "  * Completăm coloane „value” cu maximum dintre aceste 3 valori.\n",
        "  * **Bonus**, pentru cine dorește, puteți folosi și algoritmi de învățare automată, precum Linear Regression, în loc de calcularea mediei din ultimele 8 săptămâni și să comparați cele 2 metode.\n",
        "\n",
        "\n",
        "2. Calcularea valorii „received_from_grid” atunci când avem toate informațiile.\n",
        "  * Facem suma coloanelor „sent_to_ev”, „sent_to_battery” și „sent_to_grid” și scădem valorile din coloanele „received_from_pv” și „received_from_battery”.\n",
        "\n"
      ],
      "metadata": {
        "id": "wT916i56yT1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asocierea cu tarifere\n",
        "\n",
        "Curățând datele și completând valorile lipsă, putem acuma să trecem la asocierea cu tarifele consumatorilor:"
      ],
      "metadata": {
        "id": "bAyDBEYNyo9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Asocierea intrărilor de consum cu cele de tarifare, pe bază numărului de contract, al timpului și al tipului de preț, cumpărare sau vânzare.\n",
        "  * Prețul tarifat la cumpărare este prețul de cumpărare pentru intrare de consum înmulțit cu coloana „received_from_grid”\n",
        "  * Prețul tarifat la vânzare este prețul de vânzare pentru intrare de consum înmulțit cu coloana „send_to_grid”."
      ],
      "metadata": {
        "id": "ixAI6HRYyry7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcularea consumului și a facturii\n",
        "\n",
        "După asociere, putem trece la calculul factorii:\n"
      ],
      "metadata": {
        "id": "SFLIbmqzy1a1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Per zi / săptămâna / lună / an, pentru fiecare client, să se calculeze:\n",
        "  * Consumul de energie total (suma coloanei value) – „ kWh_total”\n",
        "  * Energia extrasă din baterie și cea din PV – „kWh_from_battery”, „kWh_from_PV”\n",
        "  * Energia folosită pentru EV – „kWh_for_EV”\n",
        "  * Consumul de energie folosit de la rețea – „kWh_from_grid”\n",
        "  * Costul consumului de energie folosit de la rețea – „price_billed”\n",
        "  * Consumul de energie trimis către rețea – „kWh_to_grid”\n",
        "  * Costul primit înapoi, a consumului de energie trimis către rețea – „price_cashback”\n",
        "  * Costul total (diferența de cost) – „price_final”\n",
        "  * Puneți codul de calculare al agregatelor pe orice perioadă de timp într-o funcție, iar pentru fiecare interval, zi / săptămâna / lună / an, rulați această funcție. Afișați pentru fiecare perioada top 10 clienți cu cel mai mare preț."
      ],
      "metadata": {
        "id": "t-488euxsCG_"
      }
    }
  ]
}